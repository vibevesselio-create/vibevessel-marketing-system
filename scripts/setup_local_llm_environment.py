#!/usr/bin/env python3
"""
Setup Local LLM Environment
===========================

Creates the directory structure and environment variables for the
Local LLM Stack on M2 Pro 16GB.

Usage:
    python3 scripts/setup_local_llm_environment.py [--dry-run]

This script:
1. Creates the model storage directories
2. Creates a .env.local_llm file with required environment variables
3. Creates a shell profile snippet for sourcing
4. Validates the setup
"""

import argparse
import os
import sys
from pathlib import Path


# Configuration matching services/local_llm/config.py
MODEL_ROOT = Path.home() / "Library/Application Support/SerenLocalLLM/models"
ARCHIVE_ROOT = Path("/Volumes/SYSTEM_SSD/SerenLocalLLM/models-archive")

DIRECTORIES = [
    "gguf",      # llama.cpp GGUF models
    "whisper",   # whisper.cpp models
    "hf",        # Hugging Face cache
    "indexes",   # RAG indexes
    "registry",  # Model registry JSON
    "logs",      # Service logs
]

ENV_VARS = {
    "SEREN_LLM_ROOT": str(MODEL_ROOT),
    "SEREN_LLM_ARCHIVE_ROOT": str(ARCHIVE_ROOT),
    "HF_HOME": str(MODEL_ROOT / "hf"),
    "TRANSFORMERS_CACHE": str(MODEL_ROOT / "hf"),
    "TOKENIZERS_PARALLELISM": "false",  # Avoid fork warnings
}


def create_directories(dry_run: bool = False) -> None:
    """Create required directories."""
    print("\nüìÅ Creating directories...")

    for subdir in DIRECTORIES:
        path = MODEL_ROOT / subdir
        if dry_run:
            print(f"  [DRY RUN] Would create: {path}")
        else:
            path.mkdir(parents=True, exist_ok=True)
            print(f"  ‚úÖ Created: {path}")

    # Archive root (may not exist if external drive not mounted)
    if dry_run:
        print(f"  [DRY RUN] Would create archive: {ARCHIVE_ROOT}")
    else:
        try:
            ARCHIVE_ROOT.mkdir(parents=True, exist_ok=True)
            print(f"  ‚úÖ Created archive: {ARCHIVE_ROOT}")
        except OSError as e:
            print(f"  ‚ö†Ô∏è  Could not create archive (external drive?): {e}")


def create_env_file(dry_run: bool = False) -> Path:
    """Create .env.local_llm file."""
    print("\nüìù Creating environment file...")

    project_root = Path(__file__).parent.parent
    env_file = project_root / ".env.local_llm"

    content = """# Local LLM Stack Environment Variables
# Generated by setup_local_llm_environment.py
# Source this file or add to your shell profile

"""
    for key, value in ENV_VARS.items():
        content += f'{key}="{value}"\n'

    content += """
# Optional: Ollama (only if using Ollama)
# OLLAMA_MODELS="${SEREN_LLM_ROOT}/ollama"

# Optional: XDG cache
# XDG_CACHE_HOME="${SEREN_LLM_ROOT}"
"""

    if dry_run:
        print(f"  [DRY RUN] Would write to: {env_file}")
        print("  Content:")
        for line in content.strip().split("\n"):
            print(f"    {line}")
    else:
        env_file.write_text(content)
        print(f"  ‚úÖ Created: {env_file}")

    return env_file


def create_shell_profile_snippet(dry_run: bool = False) -> None:
    """Create shell profile snippet."""
    print("\nüêö Creating shell profile snippet...")

    project_root = Path(__file__).parent.parent
    snippet_file = project_root / "scripts" / "local_llm_env.sh"

    content = """#!/bin/bash
# Local LLM Stack Environment Setup
# Add to ~/.zshrc or ~/.bashrc:
#   source /path/to/github-production/scripts/local_llm_env.sh

"""
    for key, value in ENV_VARS.items():
        content += f'export {key}="{value}"\n'

    content += """
# Verify setup
if [ -d "$SEREN_LLM_ROOT" ]; then
    echo "‚úÖ Local LLM environment loaded (root: $SEREN_LLM_ROOT)"
else
    echo "‚ö†Ô∏è  SEREN_LLM_ROOT does not exist: $SEREN_LLM_ROOT"
fi
"""

    if dry_run:
        print(f"  [DRY RUN] Would write to: {snippet_file}")
    else:
        snippet_file.write_text(content)
        snippet_file.chmod(0o755)
        print(f"  ‚úÖ Created: {snippet_file}")
        print(f"  üí° Add to your shell profile: source {snippet_file}")


def create_registry_template(dry_run: bool = False) -> None:
    """Create initial model registry template."""
    print("\nüìã Creating registry template...")

    registry_file = MODEL_ROOT / "registry" / "models.json"

    content = """{
  "version": "1.0.0",
  "updated_at": null,
  "models": {}
}
"""

    if dry_run:
        print(f"  [DRY RUN] Would write to: {registry_file}")
    else:
        if not registry_file.parent.exists():
            registry_file.parent.mkdir(parents=True, exist_ok=True)
        if not registry_file.exists():
            registry_file.write_text(content)
            print(f"  ‚úÖ Created: {registry_file}")
        else:
            print(f"  ‚è≠Ô∏è  Already exists: {registry_file}")


def validate_setup() -> bool:
    """Validate the setup."""
    print("\nüîç Validating setup...")

    all_good = True

    # Check directories
    for subdir in DIRECTORIES:
        path = MODEL_ROOT / subdir
        if path.exists():
            print(f"  ‚úÖ {subdir}/")
        else:
            print(f"  ‚ùå {subdir}/ not found")
            all_good = False

    # Check environment variables
    print("\n  Environment variables:")
    for key, expected in ENV_VARS.items():
        actual = os.getenv(key)
        if actual == expected:
            print(f"  ‚úÖ {key}")
        elif actual:
            print(f"  ‚ö†Ô∏è  {key} = {actual} (expected: {expected})")
        else:
            print(f"  ‚ùå {key} not set")
            all_good = False

    return all_good


def print_next_steps() -> None:
    """Print next steps after setup."""
    print("\n" + "=" * 60)
    print("üìã NEXT STEPS")
    print("=" * 60)
    print("""
1. Add to your shell profile (~/.zshrc or ~/.bashrc):
   source /path/to/github-production/scripts/local_llm_env.sh

2. Download models (see individual task files):
   - Task 2: llama.cpp runtime
   - Task 3: whisper.cpp runtime
   - Tasks 4-9: Model downloads

3. Start using the gateway:
   from services.local_llm import LocalLLMGateway
   gateway = LocalLLMGateway()
   response = await gateway.complete("Hello!")

4. Or use the shared_core client:
   from shared_core.local_llm import complete
   result = complete("Summarize this...")
""")


def main():
    parser = argparse.ArgumentParser(
        description="Setup Local LLM environment for M2 Pro 16GB"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Show what would be done without making changes",
    )
    parser.add_argument(
        "--validate",
        action="store_true",
        help="Only validate existing setup",
    )
    args = parser.parse_args()

    print("=" * 60)
    print("üöÄ Local LLM Environment Setup")
    print(f"   Model Root: {MODEL_ROOT}")
    print(f"   Archive Root: {ARCHIVE_ROOT}")
    print("=" * 60)

    if args.validate:
        success = validate_setup()
        sys.exit(0 if success else 1)

    if args.dry_run:
        print("\n‚ö†Ô∏è  DRY RUN MODE - No changes will be made\n")

    create_directories(args.dry_run)
    create_env_file(args.dry_run)
    create_shell_profile_snippet(args.dry_run)
    create_registry_template(args.dry_run)

    if not args.dry_run:
        validate_setup()
        print_next_steps()

    print("\n‚úÖ Setup complete!")


if __name__ == "__main__":
    main()
