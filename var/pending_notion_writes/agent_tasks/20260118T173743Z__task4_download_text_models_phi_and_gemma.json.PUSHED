{
  "type": "agent_task",
  "target_database": "Tasks",
  "target_database_id": "2e633d7a-491b-80ed-ba48-000bd4fe690e",
  "created_at": "2026-01-18T17:37:43Z",
  "source_agent": "Cursor GPT-5.2 Agent",
  "task_id": "llmstack-task-4",
  "parent_project_id": "local-llm-stack-m2pro16-v1",
  "properties": {
    "Task Name": "[TASK 4] Models — Core Text LLMs (Phi-4-mini + Gemma 2 9B) + Quantization Plan",
    "Description": "Download, validate, and register the two core text LLMs for 16GB operation.\n\n**Models:**\n- Phi-4-mini-instruct (MIT): primary router/controller + structured extraction\n- Gemma 2 9B (Gemma license): primary marketing copy/rewrite\n\n**Quantization:**\n- Prefer Q4 for Gemma 2 9B on 16GB\n- Phi-4-mini can run at higher precision if desired, but Q4/Q5 keeps memory low\n\n**Storage:**\n- Save GGUF under `$SEREN_LLM_ROOT/gguf` (preferred for llama.cpp)\n- Record license + checksum in `$SEREN_LLM_ROOT/registry/models.json`",
    "Status": "Not Started",
    "Priority": "High",
    "Owner": "Cursor GPT-5.2 Agent",
    "Tags": "Agent-Coordination, LocalLLM, Models, Text, Quantization, Agent-Task",
    "Category": "Model Acquisition"
  },
  "content": "# Models — Core Text LLMs (Phi-4-mini + Gemma 2 9B)\n\n## Objective\n\nEnsure the primary text models are locally available in 16GB-safe formats.\n\n## Target Models\n\n- Phi-4-mini-instruct (MIT): https://ai.azure.com/catalog/models/Phi-4-mini-instruct\n- Gemma 2 family overview: https://blog.google/innovation-and-ai/technology/developers-tools/google-gemma-2/\n\n## Resource Targets (16GB)\n\n- Keep peak model memory under ~10–11GB.\n- Default context settings: 4K–8K unless the model is small.\n\n## Acceptance Criteria\n\n- Both models are downloaded.\n- Preferred runtime format(s) prepared (GGUF for llama.cpp).\n- Registry entry exists for each model (path, sha256, quant, license).\n"
}
